---
title: "Product Gap Analysis"
author: "Laurence Stephan"
date: "2024-12-06"

output:
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
    fig_caption: yes
urlcolor: blue
linkcolor: orange
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# install.packages("httr")
# install.packages("jsonlite")
# install.packages("dplyr")
# install.packages("caret")
# install.packages("purrr")
# install.packages("tidyr")
# install.packages("R.utils")
# install.packages("DT")
# install.packages("stringr")
# install.packages("lubridate")
# install.packages("Matrix")
# install.packages("recommenderlab")
# install.packages("kableExtra")
# install.packages("recosystem")
# install.packages("rollama")
# install.packages("float")
# install.packages("ggplot2")
# install.packages("reshape2")
# install.packages("data.table")
# install.packages("Rtsne")
# install.packages("irlba")
```

```{r, echo=FALSE, warning=FALSE}
 library(knitr)
  opts_chunk$set(fig.path='figure/graphics-', 
                 cache.path='cache/graphics-', 
                 fig.align='center',
                 external=TRUE,
                 echo=TRUE,
                 warning=FALSE,
                 fig.pos='H'
                )
```

```{r, echo=FALSE, warning=FALSE}
# Load the configuration file.
source("config.R")

# Use the variable from the config file.
#remove all # (comments)
knitr::opts_chunk$set(comment = NA)
```


```{r, echo=FALSE, include=FALSE}
# Store the default output hook, so I can fall back to it later if needed.
default_output_hook <- knitr::knit_hooks$get("output")

# Set a custom output hook to modify how R console output is handled.
knitr::knit_hooks$set(output = function(x, options) {
  comment <- knitr::opts_current$get("comment")
  if (is.na(comment)) comment <- ""
  
  can_null <- grepl(paste0(comment, "\\s*\\[\\d?\\]"), x, perl = TRUE)
  do_null <- isTRUE(knitr::opts_current$get("null_prefix"))
  
  if (can_null && do_null) {
    align_index <- regexpr("\\]", x)[1] - 1
    
    # Remove prefix at the start of a line.
    regex <- paste0("^.{", align_index, "}\\]")
    replacementString <- comment
    x <- gsub(regex, replacementString, x)
    
    # Remove prefix after a newline.
    regex <- paste0("\\\n.{", align_index, "}\\]")
    replacementString <- paste0("\n", comment)
    x <- gsub(regex, replacementString, x)
  }
  
  # Pass modified output to the default handler.
  default_output_hook(x, options)
})

# Create an options template called "kill_prefix" that removes numeric prefixes in output.
knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

\newpage

# Introduction

This project aims to identify product market gaps using Amazon ratings data, specifically from the Toys and Games category. Amazon's website currently receives over 200 million unique visitors each month, equating to approximately 2.4 billion visits per year. In 2022, Amazon's total e-commerce sales reached around \$142.43 billion, with over 9.7 million sellers operating on the platform . E-commerce, more generally, is estimated to have a market capitalisation exceeding \$5 trillion globally, with trillions of products sold annually.

While these markets are enormous, identifying products that consumers theoretically want but don't yet have access to is a challenge that remains largely unknown to the average seller. A significant amount of time and resources is spent on activities such as market testing, proof of concept, and the development of products that ultimately fail to sell. Research suggests that about 90% of startups fail, with many of them in the e-commerce space. Thus, finding ways to anticipate consumer needs and desires to create products with higher success rates—and lower risks for sellers—could serve as an invaluable tool. Such a resource would save time, effort, money, and energy, and help reduce waste in the development process.

Furthermore, offering a tool like this to entrepreneurs, inventors, and small businesses will help reduce ambiguity when making decisions about which areas to focus on. This will enable them to concentrate on building cool products that meet the needs of the public.

To develop this tool, I employed a combination of methods. First, I performed initial pre-processing to reformat the raw JSON data into more comprehensible structures. This allowed me to explore the data to identify which information could be useful for running predictive models and to uncover trends in ratings, both overall and over time.

Then I wrangled the data to manipulate it into a format to be used across a set of machine learning algorithms, to make predictions on which products individual users are most likely to want to purchase based on the their ratings of previous products they’ve already bought. These ratings can be grouped together alongside other users who have bought a set of the same products and rated them similarly. Once we have our groups, we can recommend products to a user in a group, by looking at the products that have been well rated other users in the group, that the original user hasn’t yet purchased. The idea is if a group of people with similar purchase history to one-another have all bought an item and rated it well, but one person in the group hasn’t bought that item, they may like to buy that item, and it will probably be well received, in line with how the rest of the group felt about the item. 

In order to make these predictions I tested a small set of well known models to make predictions my test data using root mean squared error (RMSE) to understand their accuracy. I then trained and optimised the models, and chose the one with the lowest RMSE. 

Once I was able to make predictions for individual users, I extended the analysis to a larger group within the dataset to determine which products were being recommended most frequently. To interpret the results, I selected the top 50 recommendations for each user, ranked each product based on its recommendation position, and divided these rankings by the total number of users analysed. This process provided a weighted score that reflected both how often a product was recommended and its average ranking. The results were then sorted based on the highest average rankings. Additionally, I calculated the frequency with which each product appeared across the entire dataset to assess its saturation in terms of sales. Products with a high recommendation ranking but low previous sales volume were identified as having significant potential for opportunity.

\pagebreak

Another issue in society is the use of language to convey ideas, which often fails to communicate a reality beyond the subjective perspective of the observer. Over time, layers of abstract ideas and concepts have developed without rigour or a solid foundation in observation. As a result, traditional labels can become more of a hindrance than a help. One solution to this problem is to observe patterns in human behaviour and use these patterns to create new labels that are more closely related to real-world groupings, rather than relying on arbitrary labels established by historical figures in positions of influence.

To address this challenge and explore further product opportunities, I decided to examine the categories of opportunity using the latent factors uncovered in my model, in order to identify patterns of product groupings. I then reviewed the product names within those groups, and sent them to Llama2 to generate new category names.

These more specific categories are better aligned with the purchasing behaviours of real users, compared to broader labels like "outdoor toys" or "puzzle toys." The products most highly associated with these new categories potentially offer the greatest opportunity, as they are likely to be the most congruent and popular. As a result, new products designed for these categories, similar to their top-performing products, are more likely to sell well.

# Exploring the data

## Data Overview

```{r, echo=FALSE, warning=FALSE, message=FALSE}

library(httr)
library(jsonlite)
library(dplyr)
library(caret)
library(jsonlite)
library(purrr)
library(tidyr)
library(R.utils)
library(DT)
library(stringr)
library(lubridate)
library(Matrix)
library(recommenderlab)
library(kableExtra)
library(recosystem)
library(httr)
library(jsonlite)
library(rollama)
library(float)
library(ggplot2)
library(reshape2)
library(data.table)
library(Rtsne)
library(irlba)
library(gridExtra)

```

```{r, echo=FALSE, include=FALSE, eval=FALSE}

# Define save directories
toy_and_games_reviews_path <- file.path(workingDirectory, "Data", "Toys_and_Games_5.json.gz")
toy_and_games_meta_data_path <- file.path(workingDirectory, "Data", "meta_Toys_and_Games.json.gz")

# Define download URLs
toy_and_games_reviews_url <- "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Toys_and_Games_5.json.gz"
toy_and_games_meta_url <- "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/metaFiles2/meta_Toys_and_Games.json.gz"

# Download the reviews file
toy_and_games_reviews_response <- GET(toy_and_games_reviews_url, write_disk(toy_and_games_reviews_path, overwrite = TRUE))

# Download the metadata file
toy_and_games_meta_response <- GET(toy_and_games_meta_url, write_disk(toy_and_games_meta_data_path, overwrite = TRUE))

# Check if the downloads were successful
if (toy_and_games_reviews_response$status_code == 200) {
  cat("Reviews file downloaded successfully:", toy_and_games_reviews_path, "\n")
} else {
  cat("Failed to download reviews file. Status code:", toy_and_games_reviews_response$status_code, "\n")
}

if (toy_and_games_meta_response$status_code == 200) {
  cat("Metadata file downloaded successfully:", toy_and_games_meta_data_path, "\n")
} else {
  cat("Failed to download metadata file. Status code:", toy_and_games_meta_response$status_code, "\n")
}

# Unzip the files
tryCatch({
  gunzip(toy_and_games_reviews_path, overwrite = TRUE)
  cat("Unzipped reviews file successfully.\n")
}, error = function(e) {
  cat("Failed to unzip reviews file:", e$message, "\n")
})

tryCatch({
  gunzip(toy_and_games_meta_data_path, overwrite = TRUE)
  cat("Unzipped metadata file successfully.\n")
}, error = function(e) {
  cat("Failed to unzip metadata file:", e$message, "\n")
})

```

```{r, echo=FALSE, comment=NA, null_prefix=TRUE, eval=FALSE}

# Read the file line-by-line
raw_json_lines_reviews <- readLines(filepath(workingDirectory, "Data", "Toys_and_Games_5.json"))

# Parse each line and handle potential inconsistencies
json_raw_reviews <- map(raw_json_lines, function(line) {
  tryCatch({
    # Parse the JSON line
    parsed <- fromJSON(line)

    # Convert nested lists to character
    parsed_flat <- map_if(parsed, is.list, ~toString(.x))

    # Convert to a data frame
    as.data.frame(t(unlist(parsed_flat)), stringsAsFactors = FALSE)
  }, error = function(e) {
    # Print any problematic lines and return NULL
    cat("Error parsing line:", line, "\n")
    return(NULL)
  })
}) %>%
  # Remove any NULL entries from parsing errors
  discard(is.null)

# Use bind_rows to combine, which will fill missing columns with NA
json_raw_df_reviews <- bind_rows(json_raw_reviews)

# Additional flattening and cleaning
json_raw_df_flat_reviews <- json_raw_df_reviews %>%
  # Convert any remaining list columns to character
  mutate(across(where(is.list), ~ sapply(., toString)))

# Remove unnecessary 'image' columns
filtered_toys_and_games_reviews <- json_raw_df_flat_reviews %>%
  select(overall, reviewTime, reviewerID, asin, reviewerName, reviewText, summary, unixReviewTime)

# Save the flattened data frame to CSV for further processing and enhanced performance
write.csv(filtered_toys_and_games_reviews,
          "/users/laurencestephan/Programming/Product Market Gap Analysis/Data/Toys_and_Games_Review_Data_Filtered.csv",
          row.names = FALSE)

```

```{r, echo=FALSE, comment=NA, null_prefix=TRUE, eval=FALSE}

# Read the file line-by-line
raw_json_lines_meta_data <- readLines(filepath(workingDirectory, "Data", "meta_Toys_and_Games.json"))

# Parse each line and handle potential inconsistencies
json_raw_meta_data <- map(raw_json_lines_meta_data, function(line) {
  tryCatch({
    # Parse the JSON line
    parsed <- fromJSON(line)

    # Convert to a data frame, ensuring consistent structure
    as.data.frame(t(unlist(parsed)), stringsAsFactors = FALSE)
  }, error = function(e) {
    # Print the problematic lines and return NULL
    cat("Error parsing line:", line, "\n")
    return(NULL)
  })
}) %>%
  # Remove any NULL entries from parsing errors
  discard(is.null)

# Use bind_rows to combine, which will fill missing columns with NA
json_raw_df_meta_data <- bind_rows(json_raw)

# Flatten the data frame (convert nested lists to character columns)
json_raw_df_flat_meta_data <- json_raw_df_meta_data %>%
  mutate(across(where(is.list), ~ sapply(., toString)))

# Retain only the 'title', 'price', and 'asin' columns
filtered_df_meta_data <- json_raw_df_flat_meta_data %>%
  select(title, price, asin)

# Save the flattened data frame to CSV for further processing and enhanced performance
write.csv(filtered_df_meta_data, "/users/laurencestephan/Programming/Product Market Gap Analysis/Data/Toys_and_Games_Meta_Data_Filtered.csv", row.names = FALSE)

```

```{r, echo=FALSE, comment=NA, null_prefix=TRUE}

# Read in the review data
toys_and_games_reviews <- read.csv("/users/laurencestephan/Programming/Product Market Gap Analysis/Data/Toys_and_Games_Review_Data_Filtered.csv")

# Read in the meta data
toys_and_games_meta_data <- read.csv("/users/laurencestephan/Programming/Product Market Gap Analysis/Data/Toys_and_Games_Meta_Data_Filtered.csv")

# Combine the review data and metadata by asin (Amazon Standard Identification Number)
toys_and_games_combined <- toys_and_games_reviews %>%
  left_join(toys_and_games_meta_data, by = "asin", relationship = "many-to-many")

```

```{r, echo=FALSE, comment=NA, null_prefix=TRUE}
# Count rows where title is blank
blank_title_count <- toys_and_games_combined %>%
  filter(is.na(title) | title == "") %>%
  nrow()

# Count rows where asin is blank
blank_asin_count <- toys_and_games_combined %>%
  filter(is.na(asin) | asin == "") %>%
  nrow()

# Count rows where reviewerID is blank
blank_reviewerID_count <- toys_and_games_combined %>%
  filter(is.na(reviewerID) | reviewerID == "") %>%
  nrow()

# Count rows where reviewerID is blank
blank_rating_count <- toys_and_games_combined %>%
  filter(is.na(overall) | overall == "") %>%
  nrow()

# Show the counts as a dataframe
# Create a data frame for counts
counts_df <- data.frame(
  Description = c("Blank titles", 
                  "Blank asin", 
                  "Blank reviewerID", 
                  "Blank ratings", 
                  "NaN ratings", 
                  "Na ratings"),
  Count = c(blank_title_count, 
            blank_asin_count, 
            blank_reviewerID_count, 
            blank_rating_count, 
            sum(is.na(toys_and_games_combined$overall)), 
            sum(is.nan(toys_and_games_combined$overall)))
)

# Print the data frame
print(counts_df)

```
There are 2541 rows with missing titles, which is a small percentage of the data. All the other categories I need appear to be populated correctly, and my ratings data contains no Na or NAN values. To make sure my analysis remains consistent and meaningful, I will remove the rows with blank titles. 

```{r, echo=FALSE, comment=NA, null_prefix=TRUE}

# Remove rows where title is blank
toys_and_games_combined <- toys_and_games_combined %>%
  filter(!(is.na(title) | title == ""))

```

```{r, echo=FALSE, comment=NA, null_prefix=TRUE}

# Testing set will be 10% of product data.
set.seed(42)

test_index <- createDataPartition(y = toys_and_games_combined$overall, times = 1, p = 0.1, list = FALSE)

# Create the training set using rows where the indices are not in test_index.
training_set <- toys_and_games_combined[-test_index, ]

# Create the test set using rows where the indices are in test_index.
temp_testing_set <- toys_and_games_combined[test_index, ]

# Use semi_join to create a variable with the rows from the original temp dataframe and those that have matching values in both the "asin" and "reviewerId" columns from the training_set dataframe.

testing_set <- temp_testing_set %>%
  semi_join(training_set, by = "asin") %>%
  semi_join(training_set, by = "reviewerID")


# Add rows removed from final hold-out test set back into training_set set.
suppressMessages({
  removed <- anti_join(temp_testing_set, testing_set)
  training_set <- rbind(training_set, removed)
})

```

\pagebreak

```{r, echo=FALSE, comment=NA, null_prefix=TRUE}

'training data'
glimpse(training_set)

```

```{r, echo=FALSE, comment=NA, null_prefix=TRUE}

'testing data'
glimpse(testing_set)

```

$\\ \\$
The training set contains 1,696,411 rows, which make up approximately 90% of the data, while the testing set has 188,479 rows, representing about 10%. Each set contains 10 variables, represented by the columns.

The reviewerID variable represents a unique identifier assigned to each user profile. The asin variable represents a unique identifier assigned to each product. The overall variable represents the rating given by a user to a product on a scale from 1 to 5, where 1 represents the lowest score and 5 represents the highest.

The unixReviewTime and reviewTime variables act as timestamp variables that record the date and time when a product review was submitted. The title variable represents the name of the product. The reviewerName variable contains the first name or a funky nickname used by the reviewer. The price variable shows the cost of the product in dollars. Finally, the reviewText variable contains a long string of words that provide qualitative context to the review rating.

Although the same unique variables may appear across multiple rows, each combination of reviewerID and asin is unique, meaning that each row represents a distinct rating for a specific product by a specific user.

All columns are in <chr> format, except for the overall rating, which is in <int> format. For most analyses, the <chr> type data is fine. However, when running matrix factorization models, this data will need to be reformatted to work with the models.

\pagebreak
## Exploring The Data 

An initial investigation of each variable was performed to gain a strong understanding of the data.

```{r, echo=FALSE, null_prefix =TRUE}
# Create a dataframe of ratings directly from the dataset.
ratings_df <- data.frame(rating = training_set$overall)

# Plot a histogram of whole ratings with alternating colors
ggplot(ratings_df, aes(x = factor(rating))) +
  geom_bar(aes(fill = factor(rating)), color = "white", alpha = 0.7) +
  scale_x_discrete(name = "Rating", limits = as.character(1:5)) +
  scale_fill_manual(values = c("#fadf2d", "#ff9859")[1:5 %% 2 + 1]) +
  labs(
    x = "Rating",
    y = "Number of Ratings",
    caption = "Source Data: Amazon Review Data (2018)"
  ) +
  ggtitle("Rating by Number of Ratings (Whole Stars Only)") +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

```


A visual representation of the rating data shows that it only contains full star ratings. It also shows that people tend to leave higher ratings over lower ones, with the mode being 4 stars. Such findings suggest that users are more likely to leave positive reviews over negative ones.

 

```{r, echo=FALSE, null_prefix =TRUE}

# Split product genres using regular expression and arrange them by their count.
top_product_name <- training_set %>%
  group_by(title) %>%
  dplyr::summarize(count = n(), .groups = "drop") %>%
  dplyr::arrange(desc(count))

```

```{r, echo=FALSE, null_prefix =TRUE}
# Filter top 5 rows
top_product_name <- head(top_product_name, 5)

# Create the top_product_name table
top_product_table <- kable(
  top_product_name,
  format = "latex",
  booktabs = TRUE,
) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    position = "center",
    full_width = FALSE
  ) %>%
  add_header_above(c("Top Products Summary" = ncol(top_product_name))) %>%
  column_spec(1:ncol(top_product_name), width = "10cm")

```
$\\ \\$
The data was grouped by product title and arranged in descending order based on the number of ratings each product received. This table provides insight into the popularity of products, showing both the volume of ratings and which products are the most widely reviewed.

Understanding which products have received the most ratings helps guide decisions for the future. However, it’s important to note that the rating count is not adjusted for factors like website traffic or release date. Products that have been available longer naturally have more time to accumulate ratings. Additionally, the release timing of a product can influence its popularity and its potential to gain momentum. For example, products released recently may be trending and have the potential to go viral, while older products may have missed the opportunity to reach a broad audience during their initial release due to fewer users on platforms like Amazon or the internet in general.

Moreover, external factors, such as the increased internet usage and spending during lockdown periods, could have led to products released during that time outselling those released at different times. Essentially, some products may have been launched at the "wrong" time but still have future potential. Therefore, understanding the impact of release timing and website traffic bias is crucial. This highlights the value of analyzing data in time-restricted batches and utilizing real-time data to better understand trends and product potential.

```{r, echo=FALSE, null_prefix =TRUE}

# Extract the top 10 products based on the number of ratings
top_product <- training_set %>%
  group_by(title) %>%
  summarize(count = n(), .groups = "drop") %>%
  top_n(10, count) %>%
  arrange(desc(count))

# Create a bar plot using ggplot2
top_product %>%
  ggplot(aes(x = reorder(title, count), y = count)) +
  geom_bar(stat = "identity", fill = "#ff9859") +
  geom_text(aes(label = str_sub(title, start = 1, end = 60)), 
            hjust = 1, size = 2.1, color = "black") +
  coord_flip() +
  labs(
    x = "", 
    y = "Number of Ratings", 
    title = "Top 10 Products on Number of Ratings", 
    caption = "Source Data: Amazon Review Data (2018)"
  ) +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_text(size = 12),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  )

```
A visual representation of the most sold products really highlights that the 'Baby Einstein Take Along Tunes Musical Toy' is substantially popular than any other product, and in comparison, there is a much smaller gap in purchases for products 2-9. 


```{r, echo=FALSE, null_prefix =TRUE}

# Convert unixReviewTime and extract year_month
training_set_converted_date <- training_set %>%
  mutate(
    review_date = as.Date(as.POSIXct(unixReviewTime, origin = "1970-01-01")),
    year_month = format(review_date, "%Y-%m")
  )

# Compute grouped data (if not already done)
grouped_data <- training_set_converted_date %>%
  group_by(year_month) %>%
  summarise(monthly_count = n(), .groups = "drop")

# Add monthly counts to the original data
training_set_converted_date <- training_set_converted_date %>%
  left_join(grouped_data, by = "year_month")

training_set_converted_date %>%
  group_by(year_month) %>%
  summarise(
    N = n(),
    avg = mean(overall, na.rm = TRUE),
    SE = sd(overall, na.rm = TRUE) / sqrt(N)
  ) %>%
  filter(N >= 5) %>%  # Adjust threshold as needed
  slice_max(avg, n = 30) %>%  # Select the top 20 entries by avg
  mutate(year_month = reorder(year_month, avg)) %>%
  ggplot(aes(x = reorder(year_month, avg), y = avg, ymin = avg - 2 * SE, ymax = avg + 2 * SE)) +
  geom_point() +
  geom_errorbar(color = "grey", linewidth = 0.7) +
  theme_minimal() +
  labs(
    title = "Top 20 Error Bar Plots by Genres",
    caption = "Source Data: Amazon Review Data (2018)",
    x = "Titles",
    y = "Avg"
  ) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
  )

```
If I group the data by month, I can analyse which months had the highest ratings. The early 2000s seem to have a collection of months with higher average ratings, though with greater variability. In contrast, more recent months show less variability in the ratings.


```{r, echo=FALSE, null_prefix =TRUE}
# Plot 1: Histogram of ratings with count < 10,000
plot1 <- training_set %>%
  dplyr::count(asin) %>%
  filter(n < 10000) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "#ffa600", fill = "#fab937", alpha = 0.7) +
  ggtitle("Distribution of Ratings by Product") +
  labs(
    subtitle = "Number of Ratings by Product",
    x = "Number of Ratings < 10000",
    y = "Frequency",
    caption = "Source Data: Amazon Review Data (2018)"
  ) +
  theme_minimal() +
  theme(panel.border = element_rect(colour = "black", fill = NA))

# Plot 2: Histogram of ratings with log scale
plot2 <- training_set %>%
  dplyr::count(asin) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "#ffa600", fill = "#fab937", alpha = 0.7) +
  scale_x_log10() +
  ggtitle("Distribution of Ratings by Product") +
  labs(
    subtitle = "Number of Ratings by Product",
    x = "Number of Ratings (log scale)",
    y = "Frequency",
    caption = "Source Data: Amazon Review Data (2018)"
  ) +
  theme_minimal() +
  theme(panel.border = element_rect(colour = "black", fill = NA))

# Arrange the plots side by side
grid.arrange(plot1, plot2, ncol = 2)

```
\
The analysis of ratings by product reveals a highly right-skewed distribution, where most products receive very few ratings, as evidenced by the tall bar near zero, with frequency rapidly declining as the number of ratings increases. This suggests that while the majority of products have low ratings, a small subset of products are rated frequently, dominating the market. In general, certain products disproportionately outsell most others.

A second histogram, plotted on a logarithmic scale, offers a clearer view across orders of magnitude. Although the skewness is less apparent due to the log transformation, it still shows that there are significantly more products with low ratings than those with high ratings. This highlights the rarity of products with many ratings and emphasizes the data sparsity issue inherent in the dataset, making it challenging to predict product preferences accurately, especially in a market with many potentially great but unnoticed products

$\\ \\$
```{r, echo=FALSE, null_prefix =TRUE}
# Plot 1: Histogram of ratings by reviewer
plot1 <- training_set %>%
  dplyr::count(reviewerID) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "#ffa600", fill = "#fab937", alpha = 0.7) +
  ggtitle("Distribution of Ratings by Reviewer") +
  labs(
    subtitle = "Number of Ratings by Reviewer ID",
    x = "Number of Ratings",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(panel.border = element_rect(colour = "black", fill = NA))

# Plot 2: Histogram of ratings by reviewer with log scale
plot2 <- training_set %>%
  dplyr::count(reviewerID) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "#ffa600", fill = "#fab937", alpha = 0.7) +
  scale_x_log10() +
  ggtitle("Distribution of Ratings by Reviewer") +
  labs(
    subtitle = "Number of Ratings by Reviewer ID",
    x = "Number of Ratings (log scale)",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(panel.border = element_rect(colour = "black", fill = NA))

# Arrange the plots side by side
grid.arrange(plot1, plot2, ncol = 2)

```

The analysis of the number of ratings per reviewer reveals a distribution that is heavily skewed to the right, characterised by a rapid decrease in frequency as the number of ratings increases. Most reviewers leave only a few reviews, but there are some superusers who contribute over 700 reviews. This pattern suggests that while the majority of reviewers provide relatively few ratings, a small group contributes a disproportionately high number. This is unsurprising, given the ease of creating new reviewer profiles compared to consistently adding new products to the database.

This observation further highlights the critical importance of addressing scarcity in predictive modelling. It underscores challenges such as accurately predicting reviewer preferences with limited data, or when dealing with reviewers whose interests span a wide spectrum, making it difficult to find reliable nearest neighbours for predictions.

```{r, echo=FALSE, null_prefix =TRUE}

# Analyse average ratings over time by extracting weekly timestamps and calculating the mean rating for each week.
training_set_converted_date %>%
  mutate(date = round_date(as_datetime(review_date), unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(overall, na.rm = TRUE)) %>%
  ggplot(aes(date, rating)) +
  geom_point(color = "#fa4e2f", size = 0.5) +
  geom_smooth(formula = y ~ x, method = "loess", se = TRUE, color = "#fcba03", linetype = "solid") +
  ggtitle("Average Ratings Over Time") +
  labs(
    subtitle = "Timestamp, Time Unit: Week",
    x = "Date",
    y = "Average Ratings",
    caption = "Source Data: Amazon Review Data (2018)"
  ) +
  theme_minimal() +
  theme(panel.border = element_rect(colour = "#ffecbf", fill = NA))

```

When we look at average ratings over time, subdivided by week, we see unusual variability up until around 2004. There are periods when only 5-star reviews were being left, followed by periods with average ratings as low as 2. However, over time, average reviews have become more homogeneous. The early days of the internet posed many challenges, and the bizarre variability could be attributed to technical issues, bots, or problems with Amazon's data collection process.

In contrast, average ratings now strongly gravitate towards 4.5 stars. This is likely due to optimised algorithms and recommendation systems that provide reviewers with a high level of satisfaction. However, this homogeneity poses a challenge for sellers looking for gaps in the market. The high level of consistency in ratings offers less insight into products that reviewers truly like or dislike. People tend to leave positive reviews (4–5 stars) when there’s nothing wrong, but this doesn’t necessarily mean they strongly prefer the product over others.

This highlights potential issues with rating scales that lack qualitative context. For example, a 1 might mean "very dissatisfied," a 2 "dissatisfied," a 3 "as expected," a 4 "above expectations," and a 5 "completely exceeded expectations." Therefore, ratings should only be considered one part of a predictive model when analysing product market gaps.

\newpage
# Data Preprocessing

I now need to prepare my data to run my models. I will start with some simple Linear Regression models for benchmarking, and then explore a selection of matrix factorization models, which are commonly used in recommendation systems. Each of these models requires different preprocessing techniques to manipulate the data into a format that can be efficiently analysed. For Linear Regression, I'll ensure the data is clean, test for the average, and then add more variables and regularisation. For the matrix factorization models, I'll convert the data into a user-item interaction matrix, and encode the reviewer and product ID's if needed. 

## Matrix Transformation 
First I analysed the unique reviewer and prodcut ID's to understand the dimensions of my matrix. 
```{r, echo=FALSE, null_prefix =TRUE}

# Calculate the number of distinct reviewers and products in the training_set dataset.
training_set %>%
  dplyr::summarise(
    n_reviewers = n_distinct(reviewerID),
    n_products = n_distinct(asin)
  )

```
I can see that I have 208180 users, and 78695 items. Notably, as expected the amounts of users is higher higher than the amount of items, as some users have rated many products. However, given the ratio, the matrix will have a large amount of sparsity, which needs to be addressed. 



```{r, echo=FALSE, null_prefix =TRUE}

# Create a copy of the training_set data frame for manipulation without altering the original data.
training_set.copy <- training_set

# Convert the reviewerID and asin columns to factors for categorical representation.
training_set.copy$reviewerID <- as.factor(training_set.copy$reviewerID)
training_set.copy$asin <- as.factor(training_set.copy$asin)

# Convert reviewerID and asin back to numeric representation.
training_set.copy$reviewerID <- as.numeric(training_set.copy$reviewerID)
training_set.copy$asin <- as.numeric(training_set.copy$asin)

# Create a sparse matrix representation of ratings.
sparse_ratings_amazon <- sparseMatrix(
  i = training_set.copy$reviewerID,
  j = training_set.copy$asin,
  x = training_set.copy$overall,
  dims = c(
    length(unique(training_set.copy$reviewerID)),
    length(unique(training_set.copy$asin))
  ),
)

# Remove the copied data frame to free up memory.
rm(training_set.copy)

# Define constants.
num_reviewers <- 50
num_products <- 50
num_display_reviewers <- 10

# Suppress row and column names in sparse matrix display.
options(Matrix.print.rownames = FALSE)
options(Matrix.print.colnames = FALSE)

# Display a subset of the sparse ratings.
sparse_ratings_amazon[1:num_display_reviewers, 1:num_display_reviewers]

```

```{r, echo=FALSE, null_prefix =TRUE}
# Convert rating matrix into a recommenderlab sparse matrix.
ratingMat <- new("realRatingMatrix", data = sparse_ratings_amazon)
ratingMat
```

I've used the Matrix.utils package to handle my sparse matrix, which measures the cosine of the angle between two different vectors of an inner product space.



```{r, echo=FALSE, null_prefix =TRUE}

# Compute reviewer similarity using cosine similarity for the first 1000 reviewers.
similarity_reviewers <- similarity(ratingMat[1:200,], 
                               method = "cosine", 
                               which = "users")

# Visualise reviewer similarity using an image plot.
image(as.matrix(similarity_reviewers), main = "Reviewer Similarity")

# Compute product similarity using cosine similarity for the first 50 products.
similarity_products <- similarity(ratingMat[,1:50], 
                                method = "cosine", 
                                which = "items")

# Visualise product similarity using an image plot.
image(as.matrix(similarity_products), main = "Product Similarity")

```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Calculate sparsity
sparsity <- sum(ratingMat@data == 0) / prod(dim(ratingMat))

# Save the sparsity value to a file
save(sparsity, file = "sparsity_value.RData")

```

```{r, echo=FALSE, null_prefix =TRUE}
# Load the saved sparsity value
load("sparsity_value.RData")

# Print sparsity to confirm it loaded correctly
print(paste("Sparsity of the matrix:", round(sparsity * 100, 2), "%"))
```

The matrices above were generated using a subset of the first 200 reviewers to reduce computational load. Each row represents a reviewer, and each column represents a product. Consequently, each cell contains a reviewer’s rating for a product, with similarity indicated by color. Darker cells denote greater similarity between adjacent reviewers. The matrix is 99.9% sparse, which is typical of large rating-based datasets. This insight offers guidance for model testing, such as considering techniques for handling sparse data, such as matrix factorization methods. 

## Dimension Reduction
To address data sparsity, I will reduce the dimensionality using Principal Component Analysis (PCA) and Singular Value Decomposition (SVD). PCA will reduce the data’s linear dimensions, while SVD will factorise, rotate, and rescale the data to highlight key patterns.

For efficient local computation, I’ll use the IRLBA package, which is optimised for low computational load. IRLBA employs randomised algorithms based on the Lanczos method to quickly and accurately approximate the most significant eigenvalues and vectors of large, sparse matrices.

```{r, echo=FALSE, null_prefix =TRUE}
# Set seed for reproducibility.
set.seed(42)

# Perform incremental randomised SVD on the sparse_ratings matrix.
suppressMessages({
  svd_result <- irlba(sparse_ratings_amazon, tol=1e-4, verbose=TRUE, nv = 100, maxit = 1000)
})

all_sing_sq <- sum(svd_result$d^2)

```


```{r, echo=FALSE, null_prefix =TRUE}

# Set up the plotting area for 1 row and 2 columns
par(mfrow = c(1, 2))

# Plot singular values for the Reviewer-Product Matrix
plot(svd_result$d, pch=20, col = "#f7785c", cex = 0.5, xlab='Singular Value %', ylab='Magnitude', 
     main = "Singular Values for Reviewer-Product Matrix", cex.main = 0.65)

# Calculate the cumulative percentage of total sum of squares for each singular value
perc_vec <- NULL
for (i in 1:length(svd_result$d)) {
  perc_vec[i] <- sum(svd_result$d[1:i]^2) / all_sing_sq
}

# Plot the cumulative percentage against singular values and a horizontal line at 90%
plot(perc_vec, pch=20, col = "#fcc82b", cex = 0.5, xlab='Singular Values %', 
     ylab='% of Sum of Squares of Singular Values', main = "Choosing k for Dimensionality Reduction", cex.main = 0.65)
lines(x = c(0, 100), y = c(.90, .90))

```

```{r, echo=FALSE, null_prefix =TRUE}

# Calculate the percentage of total sum of squares for the first 25, 50, 75 and 90 singular values.
cat("Total Singular Value Sum (all):", all_sing_sq, "\n")

first_25 <- sum(svd_result$d[1:25]^2)
cat("Proportion of Total from First 25:", first_25 / all_sing_sq, "\n")

first_50 <- sum(svd_result$d[1:50]^2)
cat("Proportion of Total from First 50:", first_50 / all_sing_sq, "\n")

first_75 <- sum(svd_result$d[1:75]^2)
cat("Proportion of Total from First 75:", first_75 / all_sing_sq, "\n")

first_90 <- sum(svd_result$d[1:90]^2)
cat("Proportion of Total from First 90:", first_90 / all_sing_sq, "\n")


```

\pagebreak


Plotting the cumulative sum of squares for the singular values reveals that the 90% threshold is achieved with just over 80% of the singular values. Interestingly, the first 25% of the singular values only explain 18% of the total variability, with the explanation increasing gradually and uniformly. By the time we reach around 82% of the singular values, the 90% threshold is met. My goal is to identify the smallest number, 'k', of singular values whose squared sum accounts for at least 90% of the total sum of squares. This strategy ensures we capture the vast majority of variability in the data, while maintaining manageable dimensionality for further analysis, avoiding overfitting or excessive complexity in the model. Since 82% of the singular values explain 90% of the variance, we will need to keep the majority of the data, to understand variability. This means we’re retaining the core patterns in the data while simplifying the model and removing noise



```{r, echo=FALSE, null_prefix =TRUE}

# Determine the optimal value for k:
# To find k, calculate the length of the vector derived from the cumulative sum of squares.
# The chosen k corresponds to the number of singular values needed to capture 90% of the total sum of squares,
# excluding any values that exceed the 0.90 threshold.

#Find the optimal k value.
k = length(perc_vec[perc_vec <= .90])
cat("Optimal k Value:", k, "\n")

```

```{r, echo=FALSE, null_prefix =TRUE}

# Decompose Y into matrices U, D, and V.
U_k <- svd_result$u[, 1:k]
D_k <- Diagonal(x = svd_result$d[1:k])
V_k <- t(svd_result$v)[1:k, ]

# Display dimensions.
cat("Dimensions of U_k:", dim(U_k), "\n")
cat("Dimensions of D_k:", dim(D_k), "\n")
cat("Dimensions of V_k:", dim(V_k), "\n")

```
Upon observing that $k = 82$ captures 90% of the variability, I created three matrices: $D_k$ sized 82 x 82, $U_k$ sized 208180 x 82, and $V_k$ sized 82 x 78695. The total number of numeric values required to store these component matrices amounts to $(208180 \times 82) + (82 \times 82) + (82 \times 78695) = 23,530,474$. This reflects a reduction of approximately 99.9% compared to the original 16,382,725,100 entries. Despite the dimensionality reduction, I can still do more work to reduce the memory needed to run my models. To address this, I employ another reduction technique, selecting relevant data using the entire rating matrix.




## Relevant Data
While analysing the data, I noticed a pronounced left skew in both the reviewer rating counts and product ratings, revealing that much of the data offers minimal predictive power. To optimise computational efficiency without compromising the model's accuracy, I introduced a threshold for the minimum number of ratings required for both reviewers and products to be considered in the analysis.

```{r, echo=FALSE, null_prefix =TRUE}

# Determine the minimum number of products and reviewers.
min_n_products <- round(quantile(rowCounts(ratingMat), 0.90))
min_n_reviewers <- round(quantile(colCounts(ratingMat), 0.75))

cat("Minimum number of products (90th percentile):", min_n_products, "\n")
cat("Minimum number of reviewers (90th percentile):", min_n_reviewers, "\n")

# Extract ratings for products and reviewers meeting the criteria.
ratings_products <- ratingMat[
  rowCounts(ratingMat) > min_n_products,
  colCounts(ratingMat) > min_n_reviewers
]

# Display the resulting ratings matrix.
ratings_products

```
By including only products and reviewers within the 90th percentile, any products or reviewers falling within the lowest 10th percentile are excluded from the analysis. Consequently, the dataset comprises products with at least 19 ratings and reviewers who have submitted a minimum of 19 ratings. This results in a matrix featuring 17,620 distinct products and 19,658 distinct reviewers, with a total of 275,522 ratings.


\pagebreak

# Models and Results
The next section will explore several predictive models, including Linear Regression and two Matrix Factorization packages. Each model will be briefly explained, as more detailed descriptions are available elsewhere. The primary focus of this report is to identify gaps in the market. To assess the models' predictive performance, Root Mean Squared Error (RMSE) will be used, with a lower RMSE indicating better accuracy.

The equation to calculate the RMSE can be denoted as follows: $$\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }$$

```{r, echo=FALSE, null_prefix =TRUE}

#Define the RMSE function.
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}

```




## Linear Regression
To start, I will use a set of simple and straightforward linear regression models that incorporate some of the patterns identified during the data exploration process. These models serve as useful benchmarks, providing a basic comparison for more complex approaches. While they offer valuable insights, they typically don't compare directly with matrix factorization techniques, as matrix factorization can capture more complex interactions and latent factors within the data. To establish a baseline, I will begin with a very simple model using the Mean Utility (MU), which represents the average rating. Any model performing worse than this baseline will be considered highly ineffective.

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}

# Define the RMSE function.
calculate_RMSE <- function(true_ratings, predicted_ratings) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

# Calculate the global average rating.
mu <- mean(training_set$overall)
cat("Mu:", mu, "\n")

# Calculate training time and predict ratings.
training_time_mu <- system.time({
predicted_ratings <- testing_set %>%
  mutate(pred = mu) %>%
  pull(pred)
})

# Calculate RMSE.
rmse_mu <- calculate_RMSE(testing_set$overall, predicted_ratings)

# Calculate model size.
model_size_mu <- round(
  sum(
    object.size(mu),
    object.size(predicted_ratings)
  ) / (1024^2),  # Convert to MB
  4
)

# Save the results of Mu model.
saveRDS(list(rmse = rmse_mu, time = training_time_mu["elapsed"], size = model_size_mu), file = "mu_model_amazon.rds")
```

```{r, echo=FALSE, null_prefix =TRUE}
# Load model results.
mu_model <- readRDS(file.path(workingDirectory, "mu_model_amazon.rds"))

# Print results.
cat("RMSE for Mu:", mu_model$rmse, "\n")
cat("Training Time:", round(mu_model$time["elapsed"], 4), "sec\n")
cat("Model Size:", mu_model$size, "MB")
```
\makebox[3cm]{\noindent}\makebox[11cm]{\dotfill}\makebox[3cm]{\noindent}

I employed the average rating, denoted as $\mu$, as a benchmark for making predictions on the test data. In this model, each observation's prediction is simply the global average,

An RMSE of 1 will be used as the baseline for future models. This model serves as a useful benchmark, as any model exhibiting a higher error rate than this baseline will be considered ineffective and will not provide any meaningful improvements over simply predicting the average.
 
```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# product effect.
product_avgs <- training_set %>%
  group_by(asin) %>%
  dplyr::summarize(b_i = mean(overall - mu))

# Calculate training time and predict ratings.
training_time_product <- system.time({
predicted_ratings_bi <- testing_set %>%
  left_join(product_avgs, by = "asin") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
})

# Calculate RMSE for product effect.
rmse_model_product <- calculate_RMSE(testing_set$overall, predicted_ratings_bi)

# Calculate model size.
model_size_product_effect <- round(
  sum(
    object.size(product_avgs),
    object.size(predicted_ratings_bi)
  ) / (1024^2),  # Convert to MB.
  4
)

# Save the results of product effect model.
saveRDS(list(rmse = rmse_model_product, time = training_time_product["elapsed"], size = model_size_product_effect), file = "product_effect_model.rds")

```

```{r, echo=FALSE, null_prefix =TRUE}
# Load model results.
product_effect_model <- readRDS(file.path(workingDirectory, "product_effect_model.rds"))

# Print results.
cat("RMSE for product Effect:", product_effect_model$rmse, "\n")
cat("Training Time:", round(product_effect_model$time["elapsed"], 4), "sec\n")
cat("Model Size:", product_effect_model$size, "MB")
```
\makebox[3cm]{\noindent}\makebox[11cm]{\dotfill}\makebox[3cm]{\noindent}

***Product Effects***: To improve upon my original model by adding product bias to the equation denoted by $b_i$. This process enabled me to generate predictions with slightly greater precision with an RMSE score of 0.944.

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# product + reviewer effect.
reviewer_avgs <- training_set %>%
  left_join(product_avgs, by = "asin") %>%
  group_by(reviewerID) %>%
  dplyr::summarize(b_u = mean(overall - mu - b_i))

# Calculate training time and predict ratings.
training_time_product_reviewer <- system.time({
predicted_ratings_bu <- testing_set %>%
  left_join(product_avgs, by = "asin") %>%
  left_join(reviewer_avgs, by = "reviewerID") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
})

# Calculate RMSE for reviewer effect.
rmse_model_product_reviewer <- calculate_RMSE(testing_set$overall, predicted_ratings_bu)

# Calculate model size.
model_size_product_reviewer <- round(
  sum(
    object.size(product_avgs),
    object.size(reviewer_avgs),
    object.size(predicted_ratings_bu)
  ) / (1024^2),  # Convert to MB.
  4
)

# Save the results of product + reviewer effect model.
saveRDS(list(rmse = rmse_model_product_reviewer, time = training_time_product_reviewer["elapsed"], size = model_size_product_reviewer), file = "product_reviewer_effect_model.rds")

```

```{r, echo=FALSE, null_prefix =TRUE}
# Load model results.
product_reviewer_effect_model <- readRDS(file.path(workingDirectory, "product_reviewer_effect_model.rds"))

# Print results.
cat("RMSE for product + reviewer Effect:", product_reviewer_effect_model$rmse, "\n")
cat("Training Time:", round(product_reviewer_effect_model$time["elapsed"], 4), "sec\n")
cat("Model Size:", product_reviewer_effect_model$size, "MB")
```
\makebox[3cm]{\noindent}\makebox[11cm]{\dotfill}\makebox[3cm]{\noindent}

***Product + Reviewer Effects***: Next I can apply the same logic for reviewer effect, meaning the model will now take into consideration the difference between ratings from a specific reviewer depicted by $b_u$, compared to the average of all products. The new model allows me to generate predictions with a considerable improvement in precision achieving an RMSE score of 0.866.
\makebox[3cm]{\noindent}\makebox[11cm]{\dotfill}\makebox[3cm]{\noindent}
\

***Regularisation***: Next I experimented by adding regularisation to the models to try a smooth out some of the large outlier values, and stop overfitting the data.

Adjusting $λ$ allows us to strike a balance between over and under fitting the training data, while maintaining a simpler model. This balance allows us to maintain accurate pattern capture while stabilising the models behaviour, by reducing sensitivity to noise and sparse data.

This approach assumes that each reviewer's rating deviation from the mean is primarily influenced by the product they are rating. The equations can be shown as:

productID: $$\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)$$
reviewerID: $$\hat{b}_u(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu} - \hat{b}_i \right)$$. 

In contrast in the second model, I grouped by reviewerId, and then by productId. This approach assumes that each product's rating deviation from the mean is primarily influenced by the reviewer rating it. The equations can be shown as:

reviewerID: $$\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)$$
productID: $$\hat{b}_u(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu} - \hat{b}_i \right)$$. 

\pagebreak

```{r, echo=FALSE, null_prefix =TRUE}

# Regularisation.
# Set up lambda values for cross-validation.
lambdas <- seq(0, 10, 0.25)

# Initialise vectors to store RMSE and model sizes
rmses_productID <- numeric(length(lambdas))
model_size_product_reviewer_reg <- numeric(length(lambdas))

# Function to calculate RMSE with regularisation.
calculate_RMSE_reg <- function(training_set, testing_set, lambda) {
  
  mu_reg <- mean(training_set$overall)
  
  b_i_reg <- training_set %>%
    group_by(asin) %>%
    dplyr::summarize(b_i_reg = sum(overall - mu_reg) / (n() + lambda))
  
  b_u_reg <- training_set %>%
    left_join(b_i_reg, by = "asin") %>%
    group_by(reviewerID) %>%
    dplyr::summarize(b_u_reg = sum(overall - b_i_reg - mu_reg) / (n() + lambda))
  
  predicted_ratings_b_i_u <- testing_set %>%
    left_join(b_i_reg, by = "asin") %>%
    left_join(b_u_reg, by = "reviewerID") %>%
    mutate(pred = mu_reg + b_i_reg + b_u_reg) %>%
    pull(pred)
  
  model_size_product_reviewer_reg <<- round(  # Using <<- to assign it globally outside of the function. 
    sum(
      object.size(mu_reg),
      object.size(b_i_reg),
      object.size(b_u_reg),
      object.size(predicted_ratings_b_i_u)
    ) / (1024^2),  # Convert to MB.
    4
  )
  
  return(list(RMSE = RMSE(testing_set$overall, predicted_ratings_b_i_u), Model_Size = model_size_product_reviewer_reg))
}

# Calculate training time and RMSE for different lambdas using sapply.
training_time_product_reviewer_reg <- system.time({
  for (i in seq_along(lambdas)) {
    result <- calculate_RMSE_reg(training_set, testing_set, lambdas[i])
    rmses_productID[i] <- result$RMSE
    model_size_product_reviewer_reg[i] <- result$Model_Size
  }
})

# Plot RMSE values for different lambdas.
plot1 <- ggplot(data = data.frame(lambdas, rmses_productID), aes(x = lambdas, y = rmses_productID)) +
  geom_point(color = "blue") +
  geom_line(color = "red") +
  labs(
    title = "RMSE vs. Lambda",
    x = "Lambda",
    y = "RMSE"
  ) +
  theme_minimal()

```

```{r, echo=FALSE, null_prefix =TRUE}

# Initialise vectors to store RMSE and model sizes
rmses_reviewerID <- numeric(length(lambdas))
model_size_reviewer_product_reg <- numeric(length(lambdas))

# Function to calculate RMSE with regularisation.
calculate_RMSE_reg <- function(training_set, final_holdout_year, lambda) {
    
  mu_reg <- mean(training_set$overall)

    b_i_reg <- training_set %>%
      group_by(reviewerID) %>%
      dplyr::summarize(b_i_reg = sum(overall - mu_reg) / (n() + lambda))

    b_u_reg <- training_set %>%
      left_join(b_i_reg, by = "reviewerID") %>%
      group_by(asin) %>%
      dplyr::summarize(b_u_reg = sum(overall - b_i_reg - mu_reg) / (n() + lambda))

    predicted_ratings_b_i_u <- testing_set %>%
      left_join(b_i_reg, by = "reviewerID") %>%
      left_join(b_u_reg, by = "asin") %>%
      mutate(pred = mu_reg + b_i_reg + b_u_reg) %>%
      pull(pred)

   model_size_reviewer_product_reg <<- round(  # Using <<- to assign it globally outside of the function. 
    sum(
      object.size(mu_reg),
      object.size(b_i_reg),
      object.size(b_u_reg),
      object.size(predicted_ratings_b_i_u)
    ) / (1024^2),  # Convert to MB
    4
  )
  
  return(list(RMSE = RMSE(testing_set$overall, predicted_ratings_b_i_u), Model_Size = model_size_reviewer_product_reg))
}

# Calculate training time and RMSE for different lambdas using sapply.
training_time_reviewer_product_reg <- system.time({
  for (i in seq_along(lambdas)) {
    result <- calculate_RMSE_reg(training_set, testing_set, lambdas[i])
    rmses_reviewerID[i] <- result$RMSE
    model_size_reviewer_product_reg[i] <- result$Model_Size
  }
})

# Plot RMSE values for different lambdas.
plot2 <- ggplot(data = data.frame(lambdas, rmses_reviewerID), aes(x = lambdas, y = rmses_reviewerID)) +
  geom_point(color = "blue") +
  geom_line(color = "red") +
  labs(
    title = "RMSE vs. Lambda",
    x = "Lambda",
    y = "RMSE"
  ) +
  theme_minimal()

grid.arrange(plot1, plot2, ncol=2)

```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Find the optimal lambda for productID.
optimal_lambda_all <- lambdas[which.min(rmses_productID)]
cat("Optimal Lambda: ", optimal_lambda_all, "\n")

# Find the optimal lambda for reviewerID.
optimal_lambda_all <- lambdas[which.min(rmses_reviewerID)]
cat("Optimal Lambda: ", optimal_lambda_all, "\n")

# Calculate RMSE for the full model with the optimal lambda.
rmse_regularised_productID <- min(rmses_productID)

# Save the results of Regularised product + reviewer Effect model
saveRDS(list(rmse = rmse_regularised_productID, time = training_time_product_reviewer_reg["elapsed"], size = model_size_product_reviewer_reg[1]), file = "regularised_product_reviewer_effect_model.rds")

# Calculate RMSE for the full model with the optimal lambda.
rmse_regularised_reviewerID <- min(rmses_reviewerID)

# Save the results of Regularised reviewer + product effect model
saveRDS(list(rmse = rmse_regularised_reviewerID, time = training_time_reviewer_product_reg["elapsed"], size = model_size_reviewer_product_reg[1]), file = "regularised_reviewer_product_effect_model.rds")

```

```{r, echo=FALSE, null_prefix =TRUE}
# Load model results.
regularised_product_reviewer_effect_model <- readRDS(file.path(workingDirectory, "regularised_product_reviewer_effect_model.rds"))

# Print results.
cat("RMSE for Product ID with Regularisation:", regularised_product_reviewer_effect_model$rmse, "\n")
cat("Training Time:", round(regularised_product_reviewer_effect_model$time["elapsed"], 4), "sec\n")
cat("Model size:", regularised_product_reviewer_effect_model$size, "MB\n\n")

# Load model results.
regularised_reviewer_product_effect_model <- readRDS(file.path(workingDirectory, "regularised_reviewer_product_effect_model.rds"))

# Print results.
cat("RMSE for Reviewer ID with Regularisation: ", regularised_reviewer_product_effect_model$rmse, "\n")
cat("Training Time:", round(regularised_reviewer_product_effect_model$time["elapsed"], 4), "sec\n")
cat("Model size:", regularised_reviewer_product_effect_model$size, "MB", "\n")
```
I observed that applying regularization to either the Product ID or Reviewer ID variables resulted in an improvement in RMSE, reducing it to 0.88.

```{r, echo=FALSE, null_prefix =TRUE}
# Summarise the RMSE values on the validation set for the linear regression models.
rmse_results <- data.frame(
  Method = c("Mu", "Product Effect", "Product + Reviewer Effects", "Regularised Product + Reviewer Effect", "Regularised Reviewer + Product Effect"),
  
  RMSE = c(mu_model$rmse, product_effect_model$rmse, product_reviewer_effect_model$rmse, regularised_product_reviewer_effect_model$rmse, regularised_reviewer_product_effect_model$rmse),
  
  Time = c(mu_model$time, product_effect_model$time, product_reviewer_effect_model$time,  regularised_product_reviewer_effect_model$time, regularised_reviewer_product_effect_model$time),
  
  Size = c(mu_model$size, product_effect_model$size, product_reviewer_effect_model$size, regularised_product_reviewer_effect_model$size, regularised_reviewer_product_effect_model$size)
)

# Rename the columns to replace full stops with spaces.
colnames(rmse_results) <- gsub("Time", "Time (sec)", colnames(rmse_results))
colnames(rmse_results) <- gsub("Size", "Size (MB)", colnames(rmse_results))

# Display the results in a PDF-compatible table
kable(
  rmse_results, 
  format = "latex", 
  booktabs = TRUE, 
  caption = "Results for Linear Regression"
) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    position = "center"
  ) %>%
  add_header_above(c("Linear Regression" = 4))
```
This is a summary of a linear regression analysis presented in a table format. The table includes information on different regression methods, their root mean squared error (RMSE), the time taken to perform the analysis, and the size of the data or model used.

The Mu method has the highest RMSE of 1.0003386, but the fastest computation time of 0.042 seconds, and the smallest model size of 1.4381 MB. The Regularised Product + Reviewer Effect and Regularised Reviewer + Product Effect methods have the lowest RMSE values, but also the longest computation times of over 225 seconds and the largest model size of 23.3278 MB. The Product + Reviewer Effects method strikes a balance between performance (RMSE of 0.9147203) and computational requirements (0.182 seconds and 23.3278 MB).

The key reason for this is likely due to the regularisation techniques applied in these models. Regularisation adds a penalty term to the loss function, which encourages the model to learn simpler, more generalisable patterns in the data, rather than fitting too closely to the training data.

\newpage
## Recommender Engines
Now I will briefly explore the recommenderlab package, which contains a set of algorithms specifically designed for making recommendations using collaborative filtering. The package doesn't accept training and test datasets outlined in this report's confines. Instead, the functions automatically split the data. However, a brief exploration and discussion of some of its features will provide valuable insight into the realm of possibilities.

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Set the seed for reproducibility.
set.seed(42)

# Create an evaluation scheme with a 60-40 train-test split.
evaluation_scheme <- evaluationScheme(ratings_products, method = "split", train = 0.6, given = -2)

```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Create a POPULAR recommender model.
model_popular <- Recommender(getData(evaluation_scheme, "train"), 
                             method = "POPULAR")

# Make predictions on the test set.
predictions_popular <- predict(model_popular, getData(evaluation_scheme, "known"), type = "ratings")

# Calculate RMSE for the POPULAR algorithm.
rmse_popular <- calcPredictionAccuracy(predictions_popular, getData(evaluation_scheme, "unknown"))[1]

# Save the POPULAR RMSE
saveRDS(rmse_popular, file = "rmse_popular.rds")
```

```{r, echo=FALSE, null_prefix =TRUE}
# Load the POPULAR RMSE.
rmse_popular <- readRDS(file.path(workingDirectory, "rmse_popular.rds"))

# Print results.
rmse_popular
```
First, I explored the popular algorithm, which is a non-personalised approach that recommends the most popular items to all users, provided they have not rated them yet. This algorithm serves as a useful benchmark for evaluating the performance of personalised models. It achieved an RMSE score of 1.266576, which is higher than that of the MU algorithm, but it is more relevant for assessing the performance of the other models in this package.


```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Create a user-based collaborative filtering (UBCF) recommender model using Cosine similarity and 50 neighbors based on cross-validation.
set.seed(42)

user_based_collaborative_filtering_model <- Recommender(getData(evaluation_scheme, "train"),
                          method = "UBCF",
                          param = list(normalize = "center", method = "Cosine", nn = 10, shrink = 10, lambda = 0.01)

)

ubcf_prediction <- predict(user_based_collaborative_filtering_model, getData(evaluation_scheme, "known"), type = "ratings")

rmse_ubcf <- calcPredictionAccuracy(ubcf_prediction, getData(evaluation_scheme, "unknown"))[1]

# Save the UBCF RMSE
saveRDS(rmse_ubcf, file = "rmse_ubcf.rds")
```

```{r, echo=FALSE, null_prefix =TRUE}
# Load the UBCF RMSE
rmse_ubcf <- readRDS(file.path(workingDirectory, "rmse_ubcf.rds"))

# Print results.
rmse_ubcf
```
I then examined the user-based collaborative filtering algorithm within the package, which predicts ratings by averaging the ratings of users with similar rating histories to the target user. Due to computational limitations, I was only able to run the model with 10 nearest neighbours and 10% of the data. Despite these constraints, the model still yielded promising results, achieving an RMSE of 0.914213.

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Create an evaluation scheme with a 10-90 train-test split.
evaluation_scheme <- evaluationScheme(ratings_products, method = "split", train = 0.6, given = -2)

```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Create an item-based collaborative filtering (IBCF) recommender model using Cosine similarity and 10 neighbors based on cross-validation.
item_based_collaborative_filtering_model <- Recommender(getData(evaluation_scheme, "train"),
                          method = "IBCF",
                          param = list(normalize = "center", method = "Cosine", k = 200)
)

ibcf_prediction <- predict(item_based_collaborative_filtering_model, getData(evaluation_scheme, "known"), type = "ratings")

rmse_ibcf <- calcPredictionAccuracy(ibcf_prediction, getData(evaluation_scheme, "unknown"))[1]

saveRDS(rmse_ibcf, file = "rmse_ibcf.rds")
```

```{r, echo=FALSE, null_prefix =TRUE}
# Load the IBCF RMSE.
rmse_ibcf <- readRDS(file.path(workingDirectory, "rmse_ibcf.rds"))

# Print results.
rmse_ibcf
```
Finally, I investigated the item-based collaborative filtering algorithm, which involves more computationally intensive tuning parameters. These were feasible to use due to the smaller number of items compared to users. This method resulted in the lowest RMSE score of 0.8299064, marking the best performance among the models tested.

```{r, echo=FALSE, null_prefix =TRUE}

# Create the data frame for RMSE results
rmse_results_reco <- data.frame(
  Method = c("Popular", "UBCF", "IBCF"),
  Size = c(rmse_popular, rmse_ubcf, rmse_ibcf)
)

# Display the results in an Latex table using the kable function
kable(
  rmse_results_reco, 
  format = "latex", 
  booktabs = TRUE, 
  caption = "Results for Recommenderlab"
) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    position = "center"
  ) %>%
  add_header_above(c("Recommenderlab" = 2))

```
The recommenderlab package offers powerful algorithms for recommendation models that work with sparse matrices. These would be an excellent choice if the primary goal was solely to generate recommendations.

\newpage
## Matrix Factorisation
In the context of recommender systems, matrix factorization is a commonly employed technique for predicting user-item ratings. The Reco package offers models that effectively capture patterns in the data, enabling strong predictions on previously unseen datasets. Additionally, it provides a comprehensive approach to analyze the inner workings of the model, allowing us to better understand how its parameters function. This insight helps uncover latent factors, which can deepen our understanding of the data and reveal potential market opportunities or gaps.
\
```{r, echo=FALSE, null_prefix =TRUE}
## Before performing Matrix Factorisation (MF) method, clear unused memory.
invisible(gc())

# Create mapping for user and item IDs
create_id_mapping <- function(data) {
  user_map <- data.frame(
    original_id = unique(as.character(data$reviewerID)),
    numeric_id = seq_along(unique(as.character(data$reviewerID)))
  )
  
  item_map <- data.frame(
    original_id = unique(as.character(data$asin)),
    numeric_id = seq_along(unique(as.character(data$asin)))
  )
  
  list(user_map = user_map, item_map = item_map)
}

# Apply mapping to dataset
map_ids <- function(data, user_map, item_map) {
  # Create mappings
  user_lookup <- setNames(user_map$numeric_id, user_map$original_id)
  item_lookup <- setNames(item_map$numeric_id, item_map$original_id)
  
  # Replace IDs
  data$reviewerID <- as.factor(user_lookup[as.character(data$reviewerID)])
  data$asin <- as.factor(item_lookup[as.character(data$asin)])
  
  data
}

training_set.copy <- training_set %>%
  select(c("reviewerID", "asin", "overall", "title"))

testing_set.copy <- testing_set %>%
  select(c("reviewerID", "asin", "overall", "title"))

# Create ID mappings
id_mappings <- create_id_mapping(rbind(training_set.copy, testing_set.copy))

# Apply ID mapping
training_set.copy <- map_ids(training_set.copy, 
                              id_mappings$user_map, 
                              id_mappings$item_map)

testing_set.copy <- map_ids(testing_set.copy, 
                             id_mappings$user_map, 
                             id_mappings$item_map)

# Prepare matrices for file writing
training_set.copy_matrix <- as.matrix(training_set.copy)
testing_set.copy_matrix <- as.matrix(testing_set.copy)

# Write to files
write.table(training_set.copy_matrix, 
            file = "trainset.txt", 
            sep = " ", 
            row.names = FALSE, 
            col.names = FALSE, 
            quote = FALSE)

write.table(testing_set.copy_matrix, 
            file = "validset.txt", 
            sep = " ", 
            row.names = FALSE, 
            col.names = FALSE, 
            quote = FALSE)
```


```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Specify data sets from files on the hard disk using data_file().
train_set <- file.path(workingDirectory, "trainset.txt")
valid_set <- file.path(workingDirectory, "validset.txt")

# Build a Recommender object for Matrix Factorisation
recommender <- Reco()

# Optimise/tune the recommender model
opts <- recommender$tune(train_set, opts = list(
  dim = c(10, 15, 20),  # Broader dimension search
  lrate = c(0.01, 0.05, 0.1),
  nthread = 4, 
  costp_l1 = 0,
  costq_l1 = 0,
  niter = 50, 
  nfold = 5,  # Reduced folds
  verbose = FALSE
))

# Train the recommender model
recommender$train(train_set, opts = c(opts$min, nthread = 4, niter = 100, verbose = FALSE))

# Make predictions on the validation set
pred_file <- tempfile()
recommender$predict(valid_set, out_file(pred_file))

# Read actual and predicted ratings
scores_real <- read.table("validset.txt", header = FALSE, sep = " ")$V3
scores_pred <- scan(pred_file)

# Remove NA predictions
valid_indices <- !is.na(scores_pred)
scores_real_filtered <- scores_real[valid_indices]
scores_pred_filtered <- scores_pred[valid_indices]

# Calculate RMSE
rmse_mf_opt <- sqrt(mean((scores_real_filtered - scores_pred_filtered)^2))
rmse_mf_opt

```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Train the recommender model with verbose output for the first 30 iterations.
output <- capture.output(recommender$train(train_set, opts = c(opts$min, nthread = 4, niter = 30, verbose = TRUE)))

output <- output[-1]
output <- trimws(output)

# Extract data using regular expressions.
output_df <- do.call(rbind, strsplit(output, "\\s+"))
colnames(output_df) <- c("iter", "tr_rmse", "obj")

# Convert columns to appropriate types.
output_df <- as.data.frame(output_df, stringsAsFactors = FALSE)
output_df$iter <- as.integer(output_df$iter)
output_df$tr_rmse <- as.numeric(output_df$tr_rmse)
output_df$obj <- as.numeric(output_df$obj)

# Separate files for training output and model
saveRDS(recommender, "product_recommender_model.rds")
save(output_df, file = "trainRmse_product_MF.RData")
```

```{r, echo=FALSE, null_prefix =TRUE}
# Load the model.
load("trainRmse_product_MF.RData")

# Specify the iteration number for analysis.
iter.line <- 15

# Extract the training RMSE at the specified iteration.
training_rmse.line <- output_df$tr_rmse[which(output_df$iter == 15)]

# Plot the training RMSE over iterations.
suppressMessages({
  output_df %>%
    ggplot(aes(x = iter, y = tr_rmse)) +
    geom_point(size = 3, shape = 19) +
    geom_smooth(aes(x = iter, y = tr_rmse), formula = y ~ x, method = "loess") +
    geom_segment(x = 0, xend = iter.line, y = training_rmse.line, yend = training_rmse.line, color = "#f7785c", lty = 2) +
    geom_segment(x = iter.line, xend = iter.line, y = 0, yend = training_rmse.line, color = "#f7785c", lty = 2) +
    annotate(
      geom = "label", x = iter.line, y = 0.8350, color = 12,
      label = paste("x =", round(iter.line, 0), "\ny =", round(training_rmse.line, 4))
    ) +
    labs(
      title = "RMSE for different number of latent factors",
      caption = "Based on the output of r$train(train_set, opts = c(opts$min, nthread = 4, niter = 100), \n show just first 30 iterations)"
    ) +
    ylab("RMSE") +
    xlab("Latent factors")
})

```
The figure above illustrates the number of latent factors required to achieve specific RMSE values. As more latent factors are used, the computational cost of the models increases. Notably, there is a sharp decline in RMSE values until they reach approximately 1.2. Beyond this point, the decrease in RMSE slows considerably compared to the additional resources required. For instance, achieving an RMSE of 0.75 requires only 15 latent factors. This suggests an opportunity to optimize the model for greater efficiency, achieving a competitive RMSE with significantly reduced resource usage.

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Specify data sets from files on the hard disk using data_file().
train_set <- file.path(workingDirectory, "trainset.txt")
valid_set <- file.path(workingDirectory, "validset.txt")

# Build a Recommender object for Matrix Factorisation
recommender <- Reco()

# Tune the recommender with the optimal number of latent factors
opts <- recommender$tune(train_set, opts = list(
  dim = 15,  # Optimal number of latent factors from the graph
  lrate = c(0.01, 0.05, 0.1),
  nthread = 4, 
  costp_l1 = 0,
  costq_l1 = 0,
  niter = 100,  # Increase the number of iterations
  nfold = 5,
  verbose = FALSE
))
  
# Train the recommender model with the optimal configuration
recommender$train(train_set, opts = c(opts$min, nthread = 4, niter = 100, verbose = FALSE))

# Make predictions on the validation set
pred_file <- tempfile()
recommender$predict(valid_set, out_file(pred_file))

# Read actual and predicted ratings
scores_real <- read.table("validset.txt", header = FALSE, sep = " ")$V3
scores_pred <- scan(pred_file)

# Remove NA predictions
valid_indices <- !is.na(scores_pred)
scores_real_filtered <- scores_real[valid_indices]
scores_pred_filtered <- scores_pred[valid_indices]

# Calculate RMSE
rmse_mf_opt <- sqrt(mean((scores_real_filtered - scores_pred_filtered)^2))
print(paste("Final RMSE:", rmse_mf_opt))

# Save the trained recommender model
save(recommender, file = "optimised_product_recommender_model.RData")
# Save the trained recommender model to a file
saveRDS(recommender, file = "optmised_product_recommender_model.rds")

```
Using matrix factorization, I achieved an RMSE score of 0.9, which underperformed compared to my regularised linear regression models. This result may be attributed to potential overfitting of the data. While fine-tuning the parameters could lead to better performance, for the purposes of this report, the current score is sufficiently lower than that of the MU model. Additionally, matrix factorisation offers the advantage of uncovering latent factors, which can reveal valuable patterns in the data and highlight potential market gaps.

\pagebreak
# Making Predictions

```{r, echo=FALSE, null_prefix =TRUE}
# Load the saved recommender model from a file (when needed)
recommender <- readRDS("optmised_product_recommender_model.rds")

# Select a random user ID
random_user_id <- sample(id_mappings$user_map$original_id, 1)

# Get numeric ID for the random user
numeric_user_id <- id_mappings$user_map %>%
  filter(original_id == random_user_id) %>%
  pull(numeric_id)

# Prepare a list of all items for predictions
items_to_predict <- id_mappings$item_map$numeric_id

# Create a data frame for predictions
prediction_input <- data.frame(
  reviewerID = numeric_user_id,
  asin = items_to_predict
)

# Convert to matrix and save to a file
prediction_input_matrix <- as.matrix(prediction_input)
write.table(prediction_input_matrix,
            file = "prediction_input.txt",
            sep = " ",
            row.names = FALSE,
            col.names = FALSE,
            quote = FALSE)

# Generate predictions
prediction_output_file <- tempfile()

capture.output(
  recommender$predict("prediction_input.txt", out_file(prediction_output_file)),
  file = nullfile()
) 

# Read predicted scores
predicted_scores <- scan(prediction_output_file)

# Combine items and predictions
predictions <- data.frame(
  item_id = items_to_predict,
  predicted_rating = predicted_scores
)

# Map numeric item IDs back to original IDs
predictions$item_original_id <- id_mappings$item_map$original_id[
  match(predictions$item_id, id_mappings$item_map$numeric_id)
]

# Sort predictions by rating (highest to lowest)
predictions <- predictions[order(-predictions$predicted_rating), ]

# Now, perform the left join with toys_and_games_meta_data to include metadata (title, price)
predictions_with_metadata <- predictions %>%
  left_join(toys_and_games_meta_data, by = c("item_original_id" = "asin"), relationship = "many-to-many") %>%
  select(title, predicted_rating)  # Select only the relevant columns

```

```{r, echo=FALSE, null_prefix =TRUE}
# Cap titles at 60 characters and add a number column
top_10_recommendations_title <- predictions_with_metadata %>%
  head(10) %>%
  mutate(
    title = str_sub(title, 1, 60),  # Cap titles at 60 characters
    Number = seq_along(title)      # Add a sequential number column
  ) 

# Display the table with the number column
top_10_recommendations_title %>%
  select(Number, title, predicted_rating) %>%  # Select columns in the desired order
  kable("latex", col.names = c("No.", "Title", "Predicted Rating"), booktabs = TRUE) %>%
  add_header_above(setNames(3, paste("Top Picks for reviewer", random_user_id))) %>%
  kable_styling(latex_options = c("striped", "hold_position", "condensed"))

```
\
The top picks for reviewer A3KDZDJMERZ5HW feature a diverse selection of items. The highest-rated item is the Holiday Sensation Barbie - 1998 - Hallmark Gold Crown Exclusive, with a rating of 6.94, followed closely by the Wonderology – Science Kit – Pop Can Robot at 6.90, and the Mickey and Minnie - LOZ Nanoblock Disney Mickey Collection P at 6.89.

It's important to note that, due to the nature of recommendation engines and how data is scaled, review ratings may not directly correspond to the original 1-5 scale. However, the predictive capabilities of the system remain consistent. These results demonstrate that the recommendation process is functioning correctly and can be used by individuals to gain insights into their own data, specifically regarding the products they are likely to be recommended.
\
```{r, echo=FALSE, null_prefix =TRUE}
# Load the saved recommender model from a file (when needed)
recommender <- readRDS("optmised_product_recommender_model.rds")

# Prepare a list of all items for predictions (same for all users)
# items_to_predict <- id_mappings$item_map$numeric_id
# 
# # Get the total number of users
# total_users <- length(id_mappings$user_map$original_id)
# cat("Total number of users: ", total_users, "\n")
# 
# # Limit to 10% of users
# num_users_to_process <- min(20818, total_users)
# cat("Number of users being processed: ", num_users_to_process, "\n")

```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Initialize an empty data frame to store all predictions
all_predictions <- data.frame()

# Create a single output file for all users' predictions
prediction_output_file <- tempfile()  # Use only one temporary file

# Loop through the first 40 users (or total_users, whichever is smaller)
for (user_index in seq_along(id_mappings$user_map$original_id)[1:num_users_to_process]) {
  # Get the current user ID
  random_user_id <- id_mappings$user_map$original_id[user_index]

  # Print the progress for each user
  cat("Processing user ", user_index, " of ", num_users_to_process, " (User ID: ", random_user_id, ")\n")

  # Get numeric ID for the current user
  numeric_user_id <- id_mappings$user_map %>%
    filter(original_id == random_user_id) %>%
    pull(numeric_id)

  # Create a data frame for the current user's predictions
  prediction_input <- data.frame(
    reviewerID = numeric_user_id,
    asin = items_to_predict
  )

  # Convert to matrix and save to a file
  prediction_input_matrix <- as.matrix(prediction_input)
  write.table(prediction_input_matrix,
              file = "prediction_input.txt",
              sep = " ",
              row.names = FALSE,
              col.names = FALSE,
              quote = FALSE)

  # Generate predictions (suppress warnings and messages)
  suppressMessages({
    suppressWarnings({
      recommender$predict("prediction_input.txt", out_file(prediction_output_file))
    })
  })

  # Read predicted scores (suppress any potential output from scan)
  suppressMessages({
    suppressWarnings({
      predicted_scores <- scan(prediction_output_file)
    })
  })

  # Combine items and predictions for this user
  predictions <- data.frame(
    user_id = numeric_user_id,
    item_id = items_to_predict,
    predicted_rating = predicted_scores
  )

  # Sort predictions for the current user by rating (highest to lowest) and pick top 50
  inclusion_number <- 50
  
  top_predictions_for_user <- predictions[order(-predictions$predicted_rating), ][1:inclusion_number, ]
  
  # Assign weighted scores (50 for top item, 49 for second, ..., 1 for 50th)
  top_predictions_for_user$weighted_score <- (inclusion_number + 1) - seq_along(top_predictions_for_user$predicted_rating)

  # Append only the top 50 predictions with weighted scores to the overall predictions data frame
  all_predictions <- rbind(all_predictions, top_predictions_for_user)
}

# Now, calculate the total weighted score, the number of recommendations, and average rank for each item
item_weighted_scores <- all_predictions %>%
  group_by(item_id) %>%
  summarise(
    total_weighted_score = sum(weighted_score),  # Sum of weighted scores for the item
    num_recommendations = n(),  # Number of times this item appeared in top 50
    avg_ranking_position = mean(seq_along(weighted_score)),  # Average ranking position (from 1 to 50)
    top_position = min((inclusion_number + 1) - weighted_score),  # Best rank (lowest numerical position)
    lowest_position = max((inclusion_number + 1) - weighted_score),  # Worst rank (highest numerical position)
    .groups = "drop"
  )

# Calculate the average weighted score by dividing the total weighted score by the number of loops
item_weighted_scores$avg_weighted_score <- item_weighted_scores$total_weighted_score / num_users_to_process

# Map numeric item IDs back to original IDs
item_weighted_scores$item_original_id <- id_mappings$item_map$original_id[
  match(item_weighted_scores$item_id, id_mappings$item_map$numeric_id)
]

# Calculate the occurrences of each item in the original training set
item_occurrences <- training_set %>%
  group_by(asin) %>%
  summarise(occurrences = n(), .groups = "drop")

# Join the occurrences with the item_weighted_scores
final_weighted_predictions <- item_weighted_scores %>%
  left_join(toys_and_games_meta_data, by = c("item_original_id" = "asin"), relationship = "many-to-many") %>%
  left_join(item_occurrences, by = c("item_original_id" = "asin")) %>%
  arrange(-avg_weighted_score) %>%  # Order by average weighted score (highest to lowest)
  select(title, total_weighted_score, avg_weighted_score, avg_ranking_position, top_position, lowest_position, num_recommendations, occurrences)

# Number of users processed
num_users_to_process

# View the top 50 recommended products based on the weighted score
head(final_weighted_predictions)
```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Save the final weighted predictions to a CSV file
write.csv(final_weighted_predictions, 
          file = paste0("final_weighted_predictions_", num_users_to_process, "_users.csv"), 
          row.names = FALSE)
```

```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}

# Read the CSV file
grouped_predictions <- read.csv(file.path(workingDirectory, "final_weighted_predictions 20818 users.csv"))

# Compute new columns
grouped_predictions <- grouped_predictions %>%
  mutate(
    avg_position = round(50 - avg_weighted_score, 2),
    percentage_recommended = round((num_recommendations / num_users_to_process) * 100, 2),
    opportunity = round(((num_recommendations - occurrences) / num_users_to_process) * 100, 2)
  )

# Remove specified columns
grouped_predictions <- grouped_predictions %>%
  select(-total_weighted_score, -avg_weighted_score, -avg_ranking_position)

# Reorder columns
grouped_predictions <- grouped_predictions %>%
  select(
    title,
    avg_position,
    opportunity,
    top_position,
    lowest_position,
    percentage_recommended,
    num_recommendations,
    occurrences,
    everything() # Ensures other columns (if any) are retained at the end
  )

# Write updated data to CSV
write.csv(grouped_predictions, 
          file = file.path(workingDirectory, "final_weighted_predictions_users.csv"), 
          row.names = FALSE)

```

```{r, echo=FALSE, null_prefix =TRUE}
grouped_recommendations <- read.csv(file.path(workingDirectory, "final_weighted_predictions_users.csv"))

# Create the kable table
# Load grouped recommendations
grouped_recommendations <- read.csv(file.path(workingDirectory, "final_weighted_predictions_users.csv"))

# Truncate titles to a maximum of 100 characters with "..." for longer titles
grouped_recommendations <- grouped_recommendations %>%
  mutate(title = ifelse(nchar(title) > 100, 
                        paste0(substr(title, 1, 100), "..."), 
                        title))

# Create the kable table
grouped_recommendations %>% 
  arrange(desc(opportunity)) %>% 
  head(10) %>% 
  kable("latex", 
         col.names = c("Title", "Average Position", "Percentage Opportunity", "Top Position", "Lowest Position", "Percentage Recommended", "Num Recommendations", "Occurrences"), 
         align = "c", 
         booktabs = TRUE,
         # Add these options to control table and text wrapping
         longtable = FALSE,  # Prevents table from spanning multiple pages
         table.envir = "table",
         xtable.floating = TRUE,
         xtable.floating.environment = "table*",
         width = "\\textwidth"  # Make table full width of text
  ) %>% 
  add_header_above(c("Grouped Recommendations" = ncol(grouped_recommendations))) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "condensed"),
    font_size = 8,  # Reduce font size for smaller table text
    full_width = FALSE
  ) %>%
  column_spec(1, width = "3cm", latex_valign = "m") %>%  # Adjust column width for Title column
  landscape()  # Rotate the table if it spans too wide


```

To test the system, I made recommendations for individual users and then analysed 10% of the user base to identify potential market gaps. My approach involved reviewing the top 50 products recommended to users and calculating various metrics, including their average ranking positions, highest and lowest positions, the percentage of time they appeared in the top 50, and how often those products had been purchased previously. I then computed an opportunity score by subtracting the number of previous purchases from the number of recommendations for each product, divided by the number of users sampled, and scaled by 100. The rationale behind this is that products with high purchase volumes are likely well-established or in a saturated market, presenting less opportunity. Conversely, products with higher recommendation frequency and fewer past purchases represent the greatest potential. Based on this analysis, I found that Title Schleich Big Spell Set, Uncle Goose Swedish ABC Blocks Douglas and Fox Plumpie products show the most opportunity.

\pagebreak
# Latent Factors

Next, I plan to explore the latent factors generated by the model to uncover underlying patterns in user purchasing behavior. This will help identify potential groups of products that present opportunities, as well as determine which product groups have the highest overall purchase volumes.
\

```{r, echo=FALSE, null_prefix =TRUE}

# Set seed for reproducability
set.seed(42)

# Load the saved recommender model from a file (when needed)
recommender <- readRDS("optmised_product_recommender_model.rds")

# Extract the Q matrix (latent factors)
latent_factors <- recommender$model$matrices$Q@Data

# Apply t-SNE with lower perplexity
capture.output(
    tsne_result <- Rtsne(latent_factors, dims = 2, perplexity = 4, verbose = T),
  file = nullfile()
) 
```

```{r, echo=FALSE, null_prefix =TRUE}
# Plot the t-SNE result
plot(tsne_result$Y, 
     main = "t-SNE of Latent Factor Matrix", 
     xlab = "t-SNE 1", 
     ylab = "t-SNE 2", 
     pch = 19, 
     col = "orange")

```

The model was constrained to 15 latent factors in order to optimize its predictive accuracy. By visualising these latent factors, we can observe that they are widely dispersed, indicating significant differences between the groups.


```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}
# Extract the item mapping (original IDs to numeric IDs)
item_map <- id_mappings$item_map

# Access Q_matrix (latent factor matrix)
Q_matrix <- recommender$model$matrices$Q

# Remove the first column of Q_matrix if it contains only NaN values
if (all(is.na(Q_matrix[, 1]))) {
  cat("Removing the first column of Q_matrix as it contains only NaN values...\n")
  Q_matrix <- Q_matrix[, -1]
}

# Loop through all 15 latent factors
for (latent_factor_id in 1:15) {
  
  # Select the latent factor values for the current factor (e.g., latent factor 1)
  latent_factor_values <- Q_matrix[latent_factor_id, ]  # Values for the current latent factor
  
  # Convert the latent factor values to a numeric vector (in case they are not numeric)
  latent_factor_values_numeric <- as.numeric(latent_factor_values)
  
  # Create a data.table with product IDs and corresponding latent factor values
  latent_facor_dt <- data.table(
    product_id = item_map$original_id,  # Product IDs (ASINs)
    latent_value = latent_factor_values_numeric  # Numeric latent factor values for this factor
  )
  
  # Remove rows with NaN values from the data.table (if any)
  latent_facor_dt <- latent_facor_dt[!is.na(latent_value), ]
  
  # Add an index column to preserve the original positions
  latent_facor_dt[, original_index := .I]  # The original index of each product before sorting
  
  # Sort the data.table by latent factor values (in descending order)
  setorder(latent_facor_dt, -latent_value)  # Sort by latent value in decreasing order
  
  # Get the top 10 items (instead of just 5)
  top_indices <- dt[1:50, .(product_id, latent_value, original_index)]  # Top 10 items
  
  # Perform a left join with the metadata table (toys_and_games_meta_data)
  top_indices_with_meta <- left_join(top_indices, toys_and_games_meta_data, by = c("product_id" = "asin"))
  
  # Remove duplicates after the left join, if any
  top_indices_with_meta <- top_indices_with_meta[!duplicated(top_indices_with_meta$product_id), ]

  # Remove the 'price' column, if it exists
  top_indices_with_meta <- top_indices_with_meta[, !("price"), with = FALSE]
  
  # Reorder columns: title, product_id, latent_value, original_index
  top_indices_with_meta <- top_indices_with_meta[, .(title, product_id, latent_value, original_index)]
  
  # Save the result to a CSV file for the current latent factor
  file_name <- paste0(workingDirectory, "/top_indices/top_indices_latent_factor_", latent_factor_id, ".csv")

  write.csv(top_indices_with_meta, file_name, row.names = FALSE)
  
  # Print the result for the current latent factor
  cat("Top 50 products for Latent Factor", latent_factor_id, "\n")
  print(top_indices_with_meta)
  cat("\n")
}

```

The top 50 products for each latent factor are listed in the appendix. These product titles offer valuable insights into consumer purchasing patterns and help identify the themes associated with different groups. To explore these themes further and maintain a systematic approach, I will send each title to Llama3 with the following prompt:

"Here is a collection of products grouped based on their peoples purchases. Please analyse the product names and return with a detailed category name. Keep your answer under 20 words, and don’t include any of the product names. Do try and be as specific as possible, as we want category names that really capture the products meaning. Do not write any other comments, we only want the category name."


```{r, echo=FALSE, null_prefix =TRUE, eval=FALSE}

# Initialize a data frame to store cluster names
cluster_names <- data.frame(Cluster = integer(), Name = character(), stringsAsFactors = FALSE)

# Loop through all 15 latent factor files
for (latent_factor_id in 1:15) {
  # Read the CSV file for the current latent factor
  file_path <- file.path(workingDirectory, paste0("top_indices/top_indices_latent_factor_", latent_factor_id, ".csv"))
  top_10_latent_factor_products <- read.csv(file_path)
  
  # Prepare the prompt for LLaMA
  prompt <- paste(
    "Here is a collection of products grouped based on their peoples purchases",
    paste("Cluster", latent_factor_id, "items:", sep = " "),
    paste(top_10_latent_factor_products$title, collapse = "\n"),
    "\n\nPlease analyse the product names and return with a detailed category name. Keep your answer under 20 words, and don’t include any of the product names. Do try and be as specific as possible, as we want category names that really capture the products meaning. Do not write any other comments, we only want the category name."
  )
  
  # Query LLaMA and extract the response
  response <- query(prompt)  # Replace this with the appropriate LLaMA API call
  cluster_name <- paste(response[[1]]$message$content, collapse = "\n")
  
  # Append the cluster name to the data frame
  cluster_names <- rbind(cluster_names, data.frame(Cluster = latent_factor_id, Name = cluster_name, stringsAsFactors = FALSE))
}

# Save all cluster names to a CSV file
output_file <- file.path(workingDirectory, "cluster_name/cluster_names_longer.csv")
write.csv(cluster_names, output_file, row.names = FALSE)

```

```{r, echo=FALSE, null_prefix =TRUE}
# Load category names
category_names <- read.csv(file.path(workingDirectory, paste0("cluster_name/cluster_names_longer.csv")))
category_names <- category_names$Name

# Create a data frame with an index and the category names (for 1 to 15)
category_names_with_index <- data.frame(Index = 1:15, Name = category_names[1:15])


# Create the kable table with wrapped header
category_names_with_index %>%
  kable("latex", col.names = c("Index", "Category Name"), booktabs = TRUE) %>% 
  add_header_above(
    c("Category Names From Llama3" = ncol(category_names_with_index))
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "condensed")
  )

```
\pagebreak
Llama3 has generated 15 category names that capture the essence of the top 50 products within each category. These category titles provide entrepreneurs with a framework for developing products that cater to the needs of items strongly associated with the most highly recommended products in each group.

\pagebreak
# Limitations and Future Research
The ranking system in Amazon's data has two major limitations. The first is that star ratings are limited to whole numbers. Having a scale with half-star ratings or a 1-10 scale would allow for more accurate recommendations, as it provides more nuanced rating criteria. The second limitation is the skew toward more homogeneous reviews over time. A known contributor to this issue is the presence of paid reviews, fake reviews, and bots, which don't represent real customers and, therefore, distort the data. These reviews tend to be overly positive in an effort to boost sales through social proof—products with more positive reviews are perceived as more trustworthy and are more likely to sell.

Another factor contributing to this issue may be Amazon's platform optimisation. Amazon benefits from higher ratings as they lead to more sales, build greater trust in the platform, and create a self-fulfilling cycle of more positive reviews and more purchases. Positive reviews encourage repeat purchases, as prior favourable experiences are strong predictors of future behavior. As a result, Amazon’s algorithms likely promote homogeneity in ratings, making ratings themselves less useful for predictions. Instead, previous purchases combined with latent factors may serve as an effective predictive model, eliminating the need for computational resources focused on ratings.

Additional limitations stem from the processing power of my machine, which runs these models locally, as well as the need for up-to-date data. Hosting this application on scalable infrastructure with real-time data would provide valuable insights into product market gaps and enable quick decision-making. This approach could help identify emerging trends in e-commerce and allow for rapid scaling of products in time for peak seasons, such as Christmas.
